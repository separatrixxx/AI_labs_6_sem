{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.data import Dataset, AUTOTUNE\n",
        "from tensorflow import keras\n",
        "from typing import Dict, Tuple\n",
        "import re\n",
        "import keras.layers as l\n",
        "from keras import models, callbacks, utils, losses"
      ],
      "metadata": {
        "id": "EvyxcS-gmU2R"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "with open('Пикник на обочине.txt', 'r', encoding='windows-1251') as file:\n",
        "    text = file.read()\n",
        "\n",
        "def get_features_target(seq: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    features = seq[:-1]\n",
        "    target = seq[1:]\n",
        "    return features, target\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "words = list(filter(None, [re.sub('[^а-яА-ЯёЁ0-9 ,-]', '', s).strip() for s in text.split('.')]))\n",
        "alphabet = np.array(sorted(set(' '.join(words).split(' '))))\n",
        "\n",
        "word_index = {char: i for i, char in enumerate(alphabet)}\n",
        "index_word = {i: char for i, char in enumerate(alphabet)}\n",
        "\n",
        "sequences = Dataset.from_tensor_slices(np.array([word_index[word] for word in ' '.join(words).split()])).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset = sequences.map(get_features_target)\n",
        "\n",
        "data = dataset.batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
        "data = data.prefetch(AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "h808pCammmwI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),\n",
        "    l.SimpleRNN(128, return_sequences=True, stateful=True),\n",
        "    l.Dense(len(alphabet) / 2, activation='relu'),\n",
        "    l.Dense(len(alphabet))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.fit(data, epochs=50, verbose=1, steps_per_epoch= len(sequences) // BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH7Bjz_gEwU-",
        "outputId": "a59489b2-d7b1-4c6c-ed9c-34e1ed7cab8d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "44/44 [==============================] - 15s 308ms/step - loss: 8.7083 - accuracy: 0.0278\n",
            "Epoch 2/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 7.9602 - accuracy: 0.0341\n",
            "Epoch 3/50\n",
            "44/44 [==============================] - 12s 276ms/step - loss: 8.0234 - accuracy: 0.0326\n",
            "Epoch 4/50\n",
            "44/44 [==============================] - 12s 266ms/step - loss: 7.8736 - accuracy: 0.0364\n",
            "Epoch 5/50\n",
            "44/44 [==============================] - 11s 260ms/step - loss: 7.9238 - accuracy: 0.0359\n",
            "Epoch 6/50\n",
            "44/44 [==============================] - 11s 261ms/step - loss: 7.5718 - accuracy: 0.0369\n",
            "Epoch 7/50\n",
            "44/44 [==============================] - 11s 261ms/step - loss: 6.9470 - accuracy: 0.0414\n",
            "Epoch 8/50\n",
            "44/44 [==============================] - 11s 260ms/step - loss: 5.9504 - accuracy: 0.0507\n",
            "Epoch 9/50\n",
            "44/44 [==============================] - 12s 263ms/step - loss: 4.6936 - accuracy: 0.2137\n",
            "Epoch 10/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 3.9782 - accuracy: 0.3032\n",
            "Epoch 11/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 3.4443 - accuracy: 0.3693\n",
            "Epoch 12/50\n",
            "44/44 [==============================] - 12s 266ms/step - loss: 2.8802 - accuracy: 0.4399\n",
            "Epoch 13/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 2.4003 - accuracy: 0.5113\n",
            "Epoch 14/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 2.0020 - accuracy: 0.5781\n",
            "Epoch 15/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 1.6333 - accuracy: 0.6489\n",
            "Epoch 16/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 1.3308 - accuracy: 0.7086\n",
            "Epoch 17/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 1.0979 - accuracy: 0.7597\n",
            "Epoch 18/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.9030 - accuracy: 0.8041\n",
            "Epoch 19/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.7367 - accuracy: 0.8426\n",
            "Epoch 20/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.5895 - accuracy: 0.8788\n",
            "Epoch 21/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.4650 - accuracy: 0.9094\n",
            "Epoch 22/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.3646 - accuracy: 0.9331\n",
            "Epoch 23/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.2866 - accuracy: 0.9488\n",
            "Epoch 24/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.2280 - accuracy: 0.9606\n",
            "Epoch 25/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.1855 - accuracy: 0.9693\n",
            "Epoch 26/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.1556 - accuracy: 0.9728\n",
            "Epoch 27/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.1180 - accuracy: 0.9804\n",
            "Epoch 28/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0785 - accuracy: 0.9902\n",
            "Epoch 29/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0526 - accuracy: 0.9954\n",
            "Epoch 30/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0356 - accuracy: 0.9978\n",
            "Epoch 31/50\n",
            "44/44 [==============================] - 12s 266ms/step - loss: 0.0273 - accuracy: 0.9989\n",
            "Epoch 32/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0218 - accuracy: 0.9993\n",
            "Epoch 33/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.0182 - accuracy: 0.9996\n",
            "Epoch 34/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.0154 - accuracy: 0.9999\n",
            "Epoch 35/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0132 - accuracy: 0.9999\n",
            "Epoch 36/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.0099 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "44/44 [==============================] - 12s 264ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "44/44 [==============================] - 12s 265ms/step - loss: 0.0031 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dcad01b71c0>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next(sample: str, model: keras.Sequential, tokenizer: Dict[str, int], vocabulary: Dict[int, str], n_next: int, temperature: float, batch_size: int, word: bool = False) -> str:\n",
        "    if word:\n",
        "        sample_vector = [tokenizer[word] for word in sample.split()]\n",
        "    else:\n",
        "        sample_vector = [tokenizer[char] for char in sample]\n",
        "    predicted = sample_vector\n",
        "    sample_tensor = tf.expand_dims(sample_vector, 0)\n",
        "    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
        "    for i in range(n_next):\n",
        "        pred = model(sample_tensor)\n",
        "        pred = pred[0].numpy() / temperature\n",
        "        pred = tf.random.categorical(pred, num_samples=1)[-1, 0].numpy()\n",
        "        predicted.append(pred)\n",
        "        sample_tensor = predicted[-99:]\n",
        "        sample_tensor = tf.expand_dims([pred], 0)\n",
        "        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
        "    pred_seq = [vocabulary[i] for i in predicted]\n",
        "    generated = ' '.join(pred_seq) if word else ''.join(pred_seq)\n",
        "    return generated"
      ],
      "metadata": {
        "id": "qPRCuF4jtcrQ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Счастье',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=20,\n",
        "    temperature=0.6,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    word=True\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mif8PcHiGzv3",
        "outputId": "e8e3da3b-3dd5-498d-d20f-a0f10a7495e9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Счастье были говорю его какой-то себя по институт его негромко сказал Рэдрик Не несут такой Он Гуталин еще нет, еще не\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Разум',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=20,\n",
        "    temperature=0.6,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    word=True\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4BA_0irxMui",
        "outputId": "eb317c03-0d5e-4a3d-aeb5-6ec15c8abb3d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Разум у него то что был это не будем не ходил Я свое говорю, уже не было, а ты затем слова\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Шухарт',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=20,\n",
        "    temperature=0.6,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    word=True\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nbrkLYzxRPV",
        "outputId": "7d4ad326-8cba-4d95-c39f-03f77f55cfbe"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Шухарт А парень, что это за новости И бы в жизни что, только как пытается Да и Вот через сел за\n"
          ]
        }
      ]
    }
  ]
}