{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Устанавливаем необходимые библиотеки"
      ],
      "metadata": {
        "id": "5c_gv5_AIsfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install -q --upgrade keras"
      ],
      "metadata": {
        "id": "V_OL12XLor7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9d0ecf-c2d1-49d6-b222-5f9fb6156bd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m584.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем библиотеки"
      ],
      "metadata": {
        "id": "xJEaXDOxIxYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings"
      ],
      "metadata": {
        "id": "1mSoMQOoot6r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задаём параметры"
      ],
      "metadata": {
        "id": "wTwBRfjOI0ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "MIN_STRING_LEN = 512\n",
        "SEQ_LEN = 128\n",
        "\n",
        "EMBED_DIM = 256\n",
        "FEED_FORWARD_DIM = 128\n",
        "NUM_HEADS = 3\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = 5000\n",
        "\n",
        "EPOCHS = 70\n",
        "\n",
        "NUM_TOKENS_TO_GENERATE = 30"
      ],
      "metadata": {
        "id": "pAiY342RovIH"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получаем датасет и разделяем его на выборки"
      ],
      "metadata": {
        "id": "_0kTAf9qI_YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = os.path.expanduser(\"./books.txt\")\n",
        "\n",
        "raw_train_ds = (\n",
        "    tf_data.TextLineDataset(dataset_dir)\n",
        "    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(buffer_size=256)\n",
        ")\n",
        "\n",
        "raw_val_ds = (\n",
        "    tf_data.TextLineDataset(dataset_dir)\n",
        "    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "0HOrLdQDowf5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучаем токенизатор"
      ],
      "metadata": {
        "id": "OSRCpCzZJDJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    raw_train_ds,\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    lowercase=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Am0lAhNZoyH0"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем токенизатор"
      ],
      "metadata": {
        "id": "o3IlOLj0JGNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    lowercase=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Z9XldW84ozZO"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенезируем данные"
      ],
      "metadata": {
        "id": "CzZ2yJ_gJI1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_packer = keras_nlp.layers.StartEndPacker(\n",
        "    sequence_length=SEQ_LEN,\n",
        "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(inputs):\n",
        "    outputs = tokenizer(inputs)\n",
        "    features = start_packer(outputs)\n",
        "    labels = outputs\n",
        "    return features, labels\n",
        "\n",
        "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(\n",
        "    tf_data.AUTOTUNE\n",
        ")\n",
        "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(\n",
        "    tf_data.AUTOTUNE\n",
        ")"
      ],
      "metadata": {
        "id": "ikCIC-74o0Vn"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим модель"
      ],
      "metadata": {
        "id": "5qvxIZ2HJLV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")\n",
        "x = embedding_layer(inputs)\n",
        "\n",
        "for _ in range(NUM_LAYERS):\n",
        "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
        "        num_heads=NUM_HEADS,\n",
        "        intermediate_dim=FEED_FORWARD_DIM,\n",
        "    )\n",
        "    x = decoder_layer(x)\n",
        "\n",
        "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
      ],
      "metadata": {
        "id": "h8AsvIeXo1kl"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начинаем обучение модели"
      ],
      "metadata": {
        "id": "mKqSDCDmJNWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WExTupdio4aq",
        "outputId": "34cf1560-b388-4a16-adc0-ca0e73a04446"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7s/step - loss: 8.3558 - perplexity: 4295.9414 - val_loss: 7.6233 - val_perplexity: 2045.2753\n",
            "Epoch 2/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7s/step - loss: 7.2938 - perplexity: 1489.5649 - val_loss: 6.5895 - val_perplexity: 727.3959\n",
            "Epoch 3/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7s/step - loss: 6.3524 - perplexity: 578.1729 - val_loss: 5.8681 - val_perplexity: 353.5903\n",
            "Epoch 4/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - loss: 5.7858 - perplexity: 326.4159 - val_loss: 5.4702 - val_perplexity: 237.4990\n",
            "Epoch 5/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7s/step - loss: 5.4065 - perplexity: 222.9250 - val_loss: 5.3016 - val_perplexity: 200.6520\n",
            "Epoch 6/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - loss: 5.2776 - perplexity: 195.8936 - val_loss: 5.2508 - val_perplexity: 190.7144\n",
            "Epoch 7/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 5.2462 - perplexity: 189.8431 - val_loss: 5.2032 - val_perplexity: 181.8467\n",
            "Epoch 8/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6s/step - loss: 5.1819 - perplexity: 178.0476 - val_loss: 5.0401 - val_perplexity: 154.4868\n",
            "Epoch 9/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6s/step - loss: 5.0049 - perplexity: 149.1967 - val_loss: 4.8989 - val_perplexity: 134.1470\n",
            "Epoch 10/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - loss: 4.8784 - perplexity: 131.4650 - val_loss: 4.7981 - val_perplexity: 121.2764\n",
            "Epoch 11/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - loss: 4.7951 - perplexity: 120.9729 - val_loss: 4.6555 - val_perplexity: 105.1664\n",
            "Epoch 12/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7s/step - loss: 4.5480 - perplexity: 94.9716 - val_loss: 4.5276 - val_perplexity: 92.5350\n",
            "Epoch 13/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6s/step - loss: 4.5400 - perplexity: 93.7640 - val_loss: 4.3995 - val_perplexity: 81.4068\n",
            "Epoch 14/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7s/step - loss: 4.2711 - perplexity: 72.3263 - val_loss: 4.2922 - val_perplexity: 73.1252\n",
            "Epoch 15/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7s/step - loss: 4.1354 - perplexity: 63.4670 - val_loss: 4.1857 - val_perplexity: 65.7387\n",
            "Epoch 16/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7s/step - loss: 4.0176 - perplexity: 56.6465 - val_loss: 4.0891 - val_perplexity: 59.6889\n",
            "Epoch 17/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - loss: 4.0826 - perplexity: 59.4223 - val_loss: 3.9987 - val_perplexity: 54.5294\n",
            "Epoch 18/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7s/step - loss: 3.8038 - perplexity: 46.1820 - val_loss: 3.9196 - val_perplexity: 50.3822\n",
            "Epoch 19/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7s/step - loss: 3.7136 - perplexity: 42.4874 - val_loss: 3.8421 - val_perplexity: 46.6226\n",
            "Epoch 20/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - loss: 3.8356 - perplexity: 46.4709 - val_loss: 3.7661 - val_perplexity: 43.2113\n",
            "Epoch 21/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 3.8287 - perplexity: 46.1360 - val_loss: 3.6897 - val_perplexity: 40.0345\n",
            "Epoch 22/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - loss: 3.6947 - perplexity: 40.3907 - val_loss: 3.6258 - val_perplexity: 37.5553\n",
            "Epoch 23/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 3.6242 - perplexity: 37.6595 - val_loss: 3.5622 - val_perplexity: 35.2407\n",
            "Epoch 24/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - loss: 3.6290 - perplexity: 37.7983 - val_loss: 3.4923 - val_perplexity: 32.8606\n",
            "Epoch 25/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 3.4929 - perplexity: 33.0623 - val_loss: 3.4330 - val_perplexity: 30.9690\n",
            "Epoch 26/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - loss: 3.4325 - perplexity: 31.1521 - val_loss: 3.3698 - val_perplexity: 29.0731\n",
            "Epoch 27/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 3.4528 - perplexity: 31.7402 - val_loss: 3.2958 - val_perplexity: 26.9986\n",
            "Epoch 28/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6s/step - loss: 3.3840 - perplexity: 29.6412 - val_loss: 3.2271 - val_perplexity: 25.2061\n",
            "Epoch 29/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7s/step - loss: 2.8873 - perplexity: 19.8769 - val_loss: 3.1763 - val_perplexity: 23.9581\n",
            "Epoch 30/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8s/step - loss: 3.2016 - perplexity: 24.8289 - val_loss: 3.1075 - val_perplexity: 22.3654\n",
            "Epoch 31/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5s/step - loss: 3.2105 - perplexity: 24.9495 - val_loss: 3.0257 - val_perplexity: 20.6089\n",
            "Epoch 32/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 3.1208 - perplexity: 22.7987 - val_loss: 2.9552 - val_perplexity: 19.2048\n",
            "Epoch 33/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7s/step - loss: 2.5674 - perplexity: 14.8278 - val_loss: 2.8933 - val_perplexity: 18.0520\n",
            "Epoch 34/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6s/step - loss: 2.9188 - perplexity: 18.7380 - val_loss: 2.8787 - val_perplexity: 17.7912\n",
            "Epoch 35/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 2.8761 - perplexity: 17.9599 - val_loss: 2.7931 - val_perplexity: 16.3323\n",
            "Epoch 36/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6s/step - loss: 2.7937 - perplexity: 16.5315 - val_loss: 2.7177 - val_perplexity: 15.1456\n",
            "Epoch 37/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 2.8475 - perplexity: 17.3870 - val_loss: 2.6366 - val_perplexity: 13.9657\n",
            "Epoch 38/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - loss: 2.7610 - perplexity: 15.9278 - val_loss: 2.5676 - val_perplexity: 13.0350\n",
            "Epoch 39/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - loss: 2.6808 - perplexity: 14.6969 - val_loss: 2.4863 - val_perplexity: 12.0166\n",
            "Epoch 40/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 2.5884 - perplexity: 13.3998 - val_loss: 2.4289 - val_perplexity: 11.3459\n",
            "Epoch 41/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - loss: 2.4183 - perplexity: 11.3638 - val_loss: 2.3255 - val_perplexity: 10.2314\n",
            "Epoch 42/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8s/step - loss: 2.4256 - perplexity: 11.3877 - val_loss: 2.2512 - val_perplexity: 9.4994\n",
            "Epoch 43/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6s/step - loss: 2.2715 - perplexity: 9.8181 - val_loss: 2.1526 - val_perplexity: 8.6074\n",
            "Epoch 44/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6s/step - loss: 2.2780 - perplexity: 9.8402 - val_loss: 2.0606 - val_perplexity: 7.8504\n",
            "Epoch 45/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6s/step - loss: 2.0680 - perplexity: 7.9972 - val_loss: 1.9847 - val_perplexity: 7.2767\n",
            "Epoch 46/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6s/step - loss: 2.0095 - perplexity: 7.5458 - val_loss: 1.9031 - val_perplexity: 6.7068\n",
            "Epoch 47/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 9s/step - loss: 1.4983 - perplexity: 5.1049 - val_loss: 1.8313 - val_perplexity: 6.2423\n",
            "Epoch 48/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7s/step - loss: 1.4410 - perplexity: 4.8056 - val_loss: 1.7790 - val_perplexity: 5.9237\n",
            "Epoch 49/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7s/step - loss: 1.3840 - perplexity: 4.4982 - val_loss: 1.6759 - val_perplexity: 5.3434\n",
            "Epoch 50/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - loss: 1.6818 - perplexity: 5.4186 - val_loss: 1.6043 - val_perplexity: 4.9746\n",
            "Epoch 51/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5s/step - loss: 1.5957 - perplexity: 4.9656 - val_loss: 1.5270 - val_perplexity: 4.6045\n",
            "Epoch 52/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 1.5304 - perplexity: 4.6501 - val_loss: 1.4714 - val_perplexity: 4.3555\n",
            "Epoch 53/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - loss: 1.4837 - perplexity: 4.4362 - val_loss: 1.3988 - val_perplexity: 4.0502\n",
            "Epoch 54/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5s/step - loss: 1.4357 - perplexity: 4.2278 - val_loss: 1.3204 - val_perplexity: 3.7451\n",
            "Epoch 55/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6s/step - loss: 1.3470 - perplexity: 3.8680 - val_loss: 1.2655 - val_perplexity: 3.5450\n",
            "Epoch 56/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8s/step - loss: 1.3606 - perplexity: 3.9115 - val_loss: 1.2224 - val_perplexity: 3.3952\n",
            "Epoch 57/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6s/step - loss: 1.2808 - perplexity: 3.6121 - val_loss: 1.1464 - val_perplexity: 3.1468\n",
            "Epoch 58/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5s/step - loss: 1.1583 - perplexity: 3.1982 - val_loss: 1.0385 - val_perplexity: 2.8248\n",
            "Epoch 59/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5s/step - loss: 1.0609 - perplexity: 2.8987 - val_loss: 0.9918 - val_perplexity: 2.6962\n",
            "Epoch 60/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - loss: 0.7997 - perplexity: 2.3404 - val_loss: 0.9490 - val_perplexity: 2.5831\n",
            "Epoch 61/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7s/step - loss: 0.7572 - perplexity: 2.2333 - val_loss: 0.9030 - val_perplexity: 2.4669\n",
            "Epoch 62/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8s/step - loss: 0.9226 - perplexity: 2.5224 - val_loss: 0.8347 - val_perplexity: 2.3040\n",
            "Epoch 63/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6s/step - loss: 0.8663 - perplexity: 2.3838 - val_loss: 0.8012 - val_perplexity: 2.2283\n",
            "Epoch 64/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - loss: 0.8410 - perplexity: 2.3215 - val_loss: 0.7892 - val_perplexity: 2.2016\n",
            "Epoch 65/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7s/step - loss: 0.6274 - perplexity: 1.9278 - val_loss: 0.7122 - val_perplexity: 2.0386\n",
            "Epoch 66/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 0.7518 - perplexity: 2.1243 - val_loss: 0.7055 - val_perplexity: 2.0248\n",
            "Epoch 67/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6s/step - loss: 0.7307 - perplexity: 2.0785 - val_loss: 0.6790 - val_perplexity: 1.9719\n",
            "Epoch 68/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - loss: 0.7136 - perplexity: 2.0431 - val_loss: 0.6254 - val_perplexity: 1.8690\n",
            "Epoch 69/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 0.6486 - perplexity: 1.9144 - val_loss: 0.5974 - val_perplexity: 1.8174\n",
            "Epoch 70/70\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6s/step - loss: 0.6160 - perplexity: 1.8539 - val_loss: 0.5666 - val_perplexity: 1.7623\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c4489fcf2b0>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заполним нашу модель входной последовательностью, начинающейся с какого-то токена, и постепенно будем выбирать модель, делая прогнозы для каждого последующего токена в цикле"
      ],
      "metadata": {
        "id": "koKDI0JDJP3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
        "\n",
        "def next(prompt, cache, index):\n",
        "    logits = model(prompt)[:, index - 1, :]\n",
        "    hidden_states = None\n",
        "    return logits, hidden_states, cache"
      ],
      "metadata": {
        "id": "4TjmjWddo6rh"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для лучшего ответа используем Top-P search"
      ],
      "metadata": {
        "id": "BtVAu31AJicW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "\n",
        "decoded_txt = ''.join([t.decode('utf-8') for t in txt.numpy()])\n",
        "print(decoded_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvuZEtTg8Ujk",
        "outputId": "5374fd07-1b70-4c24-f115-faa929e5be65"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'position_embedding' (of type PositionEmbedding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BOS] мгновенно все преобразилось . люди отодвинулись , уплощаясь , становясь настенными изображениями ; белый стол раздался вширь , обратился в престол алтаря , где восседала в одиночестве жрица . он подошел ; кровь струилась горячо по жилам ; он все стоял и с\n"
          ]
        }
      ]
    }
  ]
}