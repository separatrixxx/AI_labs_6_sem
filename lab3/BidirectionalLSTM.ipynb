{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.data import Dataset, AUTOTUNE\n",
        "from tensorflow import keras\n",
        "from typing import Dict, Tuple\n",
        "import re\n",
        "import keras.layers as l\n",
        "from keras import models, callbacks, utils, losses"
      ],
      "metadata": {
        "id": "EvyxcS-gmU2R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "with open('Dead-souls.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "def get_features_target(seq: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    features = seq[:-1]\n",
        "    target = seq[1:]\n",
        "    return features, target\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "words = list(filter(None, [re.sub('[^а-яА-ЯёЁ0-9 ,-]', '', s).strip() for s in text.split('.')]))\n",
        "alphabet = np.array(sorted(set(' '.join(words).split(' '))))\n",
        "\n",
        "word_index = {char: i for i, char in enumerate(alphabet)}\n",
        "index_word = {i: char for i, char in enumerate(alphabet)}\n",
        "\n",
        "sequences = Dataset.from_tensor_slices(np.array([word_index[word] for word in ' '.join(words).split()])).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset = sequences.map(get_features_target)\n",
        "\n",
        "data = dataset.batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
        "data = data.prefetch(AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "h808pCammmwI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),\n",
        "    l.Bidirectional(l.LSTM(150, return_sequences=True)),\n",
        "    l.Dropout(0.2),\n",
        "    l.LSTM(512, return_sequences=True, stateful=True),\n",
        "    l.Dense(len(alphabet) / 2, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "    l.Dense(len(alphabet), activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH7Bjz_gEwU-",
        "outputId": "f1403a7d-aa02-4ca7-f7ea-9c357db8e782"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (32, None, 32)            961024    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (32, None, 300)           219600    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (32, None, 300)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (32, None, 512)           1665024   \n",
            "                                                                 \n",
            " dense (Dense)               (32, None, 15016)         7703208   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (32, None, 30032)         450990544 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 461539400 (1.72 GB)\n",
            "Trainable params: 461539400 (1.72 GB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "cuda.select_device(0)\n",
        "cuda.close()"
      ],
      "metadata": {
        "id": "FFTxt-FQvKOT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.fit(data, epochs=35, verbose=1, steps_per_epoch= len(sequences) // BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xLObM4wuUkW",
        "outputId": "b18e8656-e2ea-478d-a43b-c7ea448b5fff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "111/111 [==============================] - 116s 902ms/step - loss: 9.5546 - accuracy: 0.0343\n",
            "Epoch 2/35\n",
            "111/111 [==============================] - 100s 903ms/step - loss: 7.9430 - accuracy: 0.0369\n",
            "Epoch 3/35\n",
            "111/111 [==============================] - 100s 905ms/step - loss: 7.2157 - accuracy: 0.0546\n",
            "Epoch 4/35\n",
            "111/111 [==============================] - 100s 900ms/step - loss: 6.9939 - accuracy: 0.0698\n",
            "Epoch 5/35\n",
            "111/111 [==============================] - 99s 896ms/step - loss: 6.8292 - accuracy: 0.0870\n",
            "Epoch 6/35\n",
            "111/111 [==============================] - 99s 893ms/step - loss: 6.6331 - accuracy: 0.1056\n",
            "Epoch 7/35\n",
            "111/111 [==============================] - 99s 890ms/step - loss: 6.6098 - accuracy: 0.1155\n",
            "Epoch 8/35\n",
            "111/111 [==============================] - 98s 885ms/step - loss: 6.5945 - accuracy: 0.1240\n",
            "Epoch 9/35\n",
            "111/111 [==============================] - 98s 884ms/step - loss: 6.4964 - accuracy: 0.1315\n",
            "Epoch 10/35\n",
            "111/111 [==============================] - 98s 880ms/step - loss: 6.3513 - accuracy: 0.1366\n",
            "Epoch 11/35\n",
            "111/111 [==============================] - 97s 878ms/step - loss: 6.2273 - accuracy: 0.1421\n",
            "Epoch 12/35\n",
            "111/111 [==============================] - 97s 874ms/step - loss: 6.0580 - accuracy: 0.1484\n",
            "Epoch 13/35\n",
            "111/111 [==============================] - 97s 874ms/step - loss: 5.9128 - accuracy: 0.1610\n",
            "Epoch 14/35\n",
            "111/111 [==============================] - 97s 875ms/step - loss: 5.7377 - accuracy: 0.1746\n",
            "Epoch 15/35\n",
            "111/111 [==============================] - 97s 872ms/step - loss: 5.5465 - accuracy: 0.1959\n",
            "Epoch 16/35\n",
            "111/111 [==============================] - 97s 872ms/step - loss: 5.3519 - accuracy: 0.2171\n",
            "Epoch 17/35\n",
            "111/111 [==============================] - 97s 871ms/step - loss: 5.1364 - accuracy: 0.2373\n",
            "Epoch 18/35\n",
            "111/111 [==============================] - 96s 869ms/step - loss: 5.0130 - accuracy: 0.2582\n",
            "Epoch 19/35\n",
            "111/111 [==============================] - 97s 869ms/step - loss: 4.8225 - accuracy: 0.2722\n",
            "Epoch 20/35\n",
            "111/111 [==============================] - 97s 870ms/step - loss: 4.6811 - accuracy: 0.2847\n",
            "Epoch 21/35\n",
            "111/111 [==============================] - 96s 869ms/step - loss: 4.5257 - accuracy: 0.2954\n",
            "Epoch 22/35\n",
            "111/111 [==============================] - 96s 869ms/step - loss: 4.4134 - accuracy: 0.3069\n",
            "Epoch 23/35\n",
            "111/111 [==============================] - 96s 868ms/step - loss: 4.3278 - accuracy: 0.3162\n",
            "Epoch 24/35\n",
            "111/111 [==============================] - 97s 870ms/step - loss: 4.2415 - accuracy: 0.3147\n",
            "Epoch 25/35\n",
            "111/111 [==============================] - 96s 868ms/step - loss: 4.0453 - accuracy: 0.3449\n",
            "Epoch 26/35\n",
            "111/111 [==============================] - 96s 869ms/step - loss: 3.9288 - accuracy: 0.3580\n",
            "Epoch 27/35\n",
            "111/111 [==============================] - 96s 868ms/step - loss: 3.7945 - accuracy: 0.3766\n",
            "Epoch 28/35\n",
            "111/111 [==============================] - 96s 868ms/step - loss: 3.8003 - accuracy: 0.3765\n",
            "Epoch 29/35\n",
            "111/111 [==============================] - 96s 868ms/step - loss: 3.7033 - accuracy: 0.3948\n",
            "Epoch 30/35\n",
            "111/111 [==============================] - 96s 868ms/step - loss: 3.5783 - accuracy: 0.4137\n",
            "Epoch 31/35\n",
            "111/111 [==============================] - 96s 867ms/step - loss: 3.4517 - accuracy: 0.4299\n",
            "Epoch 32/35\n",
            "111/111 [==============================] - 96s 867ms/step - loss: 3.5130 - accuracy: 0.4181\n",
            "Epoch 33/35\n",
            "111/111 [==============================] - 96s 867ms/step - loss: 3.5486 - accuracy: 0.4130\n",
            "Epoch 34/35\n",
            "111/111 [==============================] - 96s 867ms/step - loss: 3.4442 - accuracy: 0.4274\n",
            "Epoch 35/35\n",
            "111/111 [==============================] - 96s 867ms/step - loss: 3.2851 - accuracy: 0.4595\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79242b94bfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next(sample: str, model: keras.Sequential, tokenizer: Dict[str, int], vocabulary: Dict[int, str], n_next: int, temperature: float, batch_size: int, word: bool = False) -> str:\n",
        "    if word:\n",
        "        sample_vector = [tokenizer[word] for word in sample.split()]\n",
        "    else:\n",
        "        sample_vector = [tokenizer[char] for char in sample]\n",
        "    predicted = sample_vector\n",
        "    sample_tensor = tf.expand_dims(sample_vector, 0)\n",
        "    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
        "    for i in range(n_next):\n",
        "        pred = model(sample_tensor)\n",
        "        pred = pred[0].numpy() / temperature\n",
        "        pred = tf.random.categorical(pred, num_samples=1)[-1, 0].numpy()\n",
        "        predicted.append(pred)\n",
        "        sample_tensor = predicted[-99:]\n",
        "        sample_tensor = tf.expand_dims([pred], 0)\n",
        "        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
        "    pred_seq = [vocabulary[i] for i in predicted]\n",
        "    generated = ' '.join(pred_seq) if word else ''.join(pred_seq)\n",
        "    return generated"
      ],
      "metadata": {
        "id": "qPRCuF4jtcrQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Где',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=20,\n",
        "    temperature=0.6,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    word=True\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mif8PcHiGzv3",
        "outputId": "ed8747ce-ac41-4e21-a152-02fe824bd790"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Где разбойничье поднят тверже раздевать подсвечнике неведомый изумления вышить Ребята, неблагоприятных изобрел фигурка скачки, правую фаянсовых сказанное правильные опасности, лавки досаду,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Душ',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=20,\n",
        "    temperature=0.6,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    word=True\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4BA_0irxMui",
        "outputId": "6a6cfb3f-a2ea-42c3-eacb-df31e8ba81fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Душ бриться, гражданского Эге шахматы спаси приятели, первых-то молоденькую закопался воздвигнуть Изумляются молоденькие туда ризе положила ни, каурой грамотно Экой надлежащих\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Поручик',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=20,\n",
        "    temperature=0.6,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    word=True\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nbrkLYzxRPV",
        "outputId": "ad4120ee-3db6-4fc8-b4d6-e26336750c6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Поручик устремлено сердито, похлопотать, однако танцевавшее разговора, судьбамиЧичиков прыть поворачивать наплетет, зеркала выехал Петух распечет узел сап чувствами сором пристроил палец\n"
          ]
        }
      ]
    }
  ]
}