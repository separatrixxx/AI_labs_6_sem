{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.data import Dataset, AUTOTUNE\n",
        "from tensorflow import keras\n",
        "from typing import Dict, Tuple\n",
        "import keras.layers as l\n",
        "from keras import models, callbacks, utils, losses"
      ],
      "metadata": {
        "id": "EvyxcS-gmU2R"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "with open('Пикник на обочине.txt', 'r', encoding='windows-1251') as file:\n",
        "    text = file.read()\n",
        "\n",
        "def get_features_target(seq: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    features = seq[:-1]\n",
        "    target = seq[1:]\n",
        "    return features, target\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "alphabet = np.array(sorted(set(text)))\n",
        "\n",
        "word_index = {char: i for i, char in enumerate(alphabet)}\n",
        "index_word = {i: char for i, char in enumerate(alphabet)}\n",
        "\n",
        "sequences = Dataset.from_tensor_slices(np.array([word_index[char] for char in text])).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset = sequences.map(get_features_target)\n",
        "\n",
        "data = dataset.batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
        "data = data.prefetch(AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "h808pCammmwI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),\n",
        "    l.SimpleRNN(512, return_sequences=True, stateful=True),\n",
        "    l.SimpleRNN(512, return_sequences=True, stateful=True),\n",
        "    l.Dense(len(alphabet))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.fit(data, epochs=20, verbose=1, steps_per_epoch= len(sequences) // BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH7Bjz_gEwU-",
        "outputId": "de785e2f-85cd-4776-c11f-ac006667cdd7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "29/29 [==============================] - 8s 190ms/step - loss: 3.5125 - accuracy: 0.1353\n",
            "Epoch 2/20\n",
            "29/29 [==============================] - 7s 228ms/step - loss: 3.3477 - accuracy: 0.1503\n",
            "Epoch 3/20\n",
            "29/29 [==============================] - 5s 156ms/step - loss: 3.3387 - accuracy: 0.1503\n",
            "Epoch 4/20\n",
            "29/29 [==============================] - 4s 155ms/step - loss: 3.2261 - accuracy: 0.1677\n",
            "Epoch 5/20\n",
            "29/29 [==============================] - 6s 212ms/step - loss: 2.8956 - accuracy: 0.2132\n",
            "Epoch 6/20\n",
            "29/29 [==============================] - 4s 155ms/step - loss: 2.9125 - accuracy: 0.2201\n",
            "Epoch 7/20\n",
            "29/29 [==============================] - 5s 187ms/step - loss: 2.7526 - accuracy: 0.2387\n",
            "Epoch 8/20\n",
            "29/29 [==============================] - 5s 174ms/step - loss: 2.6093 - accuracy: 0.2593\n",
            "Epoch 9/20\n",
            "29/29 [==============================] - 4s 154ms/step - loss: 2.5498 - accuracy: 0.2738\n",
            "Epoch 10/20\n",
            "29/29 [==============================] - 6s 209ms/step - loss: 2.5000 - accuracy: 0.2863\n",
            "Epoch 11/20\n",
            "29/29 [==============================] - 5s 158ms/step - loss: 2.8386 - accuracy: 0.2371\n",
            "Epoch 12/20\n",
            "29/29 [==============================] - 5s 159ms/step - loss: 2.5924 - accuracy: 0.2646\n",
            "Epoch 13/20\n",
            "29/29 [==============================] - 6s 208ms/step - loss: 2.4950 - accuracy: 0.2854\n",
            "Epoch 14/20\n",
            "29/29 [==============================] - 5s 158ms/step - loss: 2.4494 - accuracy: 0.2955\n",
            "Epoch 15/20\n",
            "29/29 [==============================] - 6s 204ms/step - loss: 2.4168 - accuracy: 0.3032\n",
            "Epoch 16/20\n",
            "29/29 [==============================] - 5s 157ms/step - loss: 2.3901 - accuracy: 0.3108\n",
            "Epoch 17/20\n",
            "29/29 [==============================] - 5s 158ms/step - loss: 2.3635 - accuracy: 0.3179\n",
            "Epoch 18/20\n",
            "29/29 [==============================] - 7s 230ms/step - loss: 2.3383 - accuracy: 0.3249\n",
            "Epoch 19/20\n",
            "29/29 [==============================] - 5s 157ms/step - loss: 2.3153 - accuracy: 0.3299\n",
            "Epoch 20/20\n",
            "29/29 [==============================] - 6s 203ms/step - loss: 2.2923 - accuracy: 0.3357\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dcac875ac20>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next(sample: str, model: keras.Sequential, tokenizer: Dict[str, int], vocabulary: Dict[int, str], n_next: int, temperature: float, batch_size: int, word: bool = False) -> str:\n",
        "    if word:\n",
        "        sample_vector = [tokenizer[word] for word in sample.split()]\n",
        "    else:\n",
        "        sample_vector = [tokenizer[char] for char in sample]\n",
        "    predicted = sample_vector\n",
        "    sample_tensor = tf.expand_dims(sample_vector, 0)\n",
        "    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
        "    for i in range(n_next):\n",
        "        pred = model(sample_tensor)\n",
        "        pred = pred[0].numpy() / temperature\n",
        "        pred = tf.random.categorical(pred, num_samples=1)[-1, 0].numpy()\n",
        "        predicted.append(pred)\n",
        "        sample_tensor = predicted[-99:]\n",
        "        sample_tensor = tf.expand_dims([pred], 0)\n",
        "        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
        "    pred_seq = [vocabulary[i] for i in predicted]\n",
        "    generated = ' '.join(pred_seq) if word else ''.join(pred_seq)\n",
        "    return generated"
      ],
      "metadata": {
        "id": "qPRCuF4jtcrQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Разум',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=200,\n",
        "    temperature=0.6,\n",
        "    batch_size=BATCH_SIZE\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mif8PcHiGzv3",
        "outputId": "25286483-8816-497e-dc4c-9f4fa44b9560"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Разум, говорат и поперал водота, полек посесь постовал ого не в подло на сограсто и свомо и дело воребно, постом бень и в это это стацо на подноже увитальи. Дак замат вез дыль прозал, как там волного в дад\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Сердце',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=100,\n",
        "    temperature=0.2,\n",
        "    batch_size=BATCH_SIZE\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4BA_0irxMui",
        "outputId": "4abe3fee-6cca-4f00-d4a0-4bac3178add9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сердце стал продул в постовал с не стал в сказал он в сторал он подерал волько в стал в это выл он в сто м\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Боль',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=150,\n",
        "    temperature=0.81,\n",
        "    batch_size=BATCH_SIZE\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nbrkLYzxRPV",
        "outputId": "132656f7-10c7-4225-8dcd-0ad90b14793f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Боль. Вак нем. Я нак егрыгадь к м горани и скошки протникомняю, плесди ко поволя и вак коже смабалы дал? Нучамы. Рэдола ведо, посерик, на тебоди, разрерну\n"
          ]
        }
      ]
    }
  ]
}