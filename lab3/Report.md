## Отчёт по лабораторной работе

Выполнили:


Студент (ФИО) | Роль в проекте   | Оценка
-------------|---------------------|------
Лохматов Никита Игоревич | обучал GPT с нуля и дообучал GPT | 
Мезенин Олег Александрович | LSTM | 
Чапкин Владислав Вячеславович | Двунаправленная LSTM | 
Синюков Антон Сергеевич | SimpleRNN | 

## SimpleRNN

Для тренировки использовалась повесть братьев Стругацких «Пикник на обочине».

Для побуквенной токенизации использовалась следующая конфигурация слоев:
```py
model = keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    l.SimpleRNN(512, return_sequences=True, stateful=True),
    l.SimpleRNN(512, return_sequences=True, stateful=True),
    l.Dense(len(alphabet))
])
```
Обучение проходило на 20 эпохах, алгоритм оптимизации - Adam.

Результаты следующие:
>Разум, говорат и поперал водота, полек посесь постовал ого не в подло на сограсто и свомо и дело воребно, постом бень и в это это стацо на подноже увитальи. Дак замат вез дыль прозал, как там волного в дад

>Сердце стал продул в постовал с не стал в сказал он в сторал он подерал волько в стал в это выл он в сто м

>Боль. Вак нем. Я нак егрыгадь к м горани и скошки протникомняю, плесди ко поволя и вак коже смабалы дал? Нучамы. Рэдола ведо, посерик, на тебоди, разрерну

Выглядит все так, как будто нейронная сеть пытается придумать новые слова, увидев как составляются они в книге. Зачастую после гласной буквы идет согласная, иногда даже прослеживаются существующие слова. Наверное, результат был бы лучше, если бы алгоритм прошел через большее число эпох, либо если усложнить конфигурацию слоев. Также можно увеличить размер датасета (1 книга не особо много).

Для пословной токенизации использовалась следующая конфигурация слоев:
```py
model = keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    l.SimpleRNN(128, return_sequences=True, stateful=True),
    l.Dense(len(alphabet) / 2, activation='relu'),
    l.Dense(len(alphabet))
])
```
Обучение проходило на 50 эпохах, алгоритм оптимизации - Adam.

Результаты следующие:
>Счастье были говорю его какой-то себя по институт его негромко сказал Рэдрик Не несут такой Он Гуталин еще нет, еще не

>Разум у него то что был это не будем не ходил Я свое говорю, уже не было, а ты затем слова

>Шухарт А парень, что это за новости И бы в жизни что, только как пытается Да и Вот через сел за

Теперь у нейронной сети в распоряжении все слова из книги, благодаря чему не возникнут несуществующие слова, но и не возникнут существующие, которых нет в книге. Теперь нейронная сеть учится составлять из этих слов предложения, и получается это лучше чем у предыдущей. Можно заметить, что нейронная сеть ставит запятые зачастую после союзов, после глагола хочет поставить имя (или слово с заглавной буквы). Но при этом смысла у таких предложений как не было так и нет.

## LSTM

Для обучения использовалась русская литература с сайта lib.ru (2 млн символов).

### Токенизация по символам

Для токенизации по символам использовалась модель следующей конфигурации
```py
model = keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    keras.layers.LSTM(512, return_sequences=True, stateful=True),
    keras.layers.LSTM(512, return_sequences=True, stateful=True),
    l.Dense(len(alphabet))
])
```

Обучение происходило на 65 эпохах, для оптимизации использовался Adam.

Примеры генерации на обученной модели:

>что-нибудь в тот дело, когда-нибудь страх? - Мне с ним поднимаются. Теперь уже не хотелось. Он собрал объединиваться на две горячего голово?.. - С десять циновой проектов, давай еред отраженное разговору

>где бандитская глаза - просто он просто стоит запотевшего старые рассуждения ничего не выходит на две п

>когда из-за лампочка, хотя и не позвонил вам, - его терзала. Я бы тоже предполагал, одновременно скользил головой, больше ничего. - Нет, постой, одно двена

### Токенизация по словам

Для токенизации по словам использовалась модель следующей конфигурации
```py
model = keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    l.LSTM(512, return_sequences=True, stateful=True),
    l.Dense(len(alphabet) / 2, activation='relu'),
    l.Dense(len(alphabet))
])
```

Обучение происходило на 35 эпохах, для оптимизации использовался Adam.

Примеры генерации на обученной модели:

>Что же девушка в руках Но тот самый спичечный коробок, ради колени и под этими листьями обнаженная женщина, снаружи остаются лишь руки и ноги налетает ветер, и открывается лицо и лицо это превращается неожиданно на лицо он уже не от ног Меня Смысл Однако тот самый добычу меня с пианино и это делает их Или же хоть ее для нее в этом случае, который не

>Где я не думаю, что он не в силах сомкнуть глаз, поэтому хотя я проснулся совсем недавно, так и не смог выспаться как следует Но это время еще ничего когда я проснулся, то обнаружил, что белье и брюки в два с половиной раза в нем установлено множество телефонов-автоматов По рассказам очевидцев, этот человек примерно полдня сидел в одной и той же позе, но в течение

>Когда если ты не был на свою силы и, чтобы скрыть свое вины, безвозвратно один человек, и с таким смешением Предвкушая жареную креветку, человек выплюнет еду и меня он с такой глупости, чтобы, в нее отходы только потому, что в него нет Если бы было ни денег, то, может быть, мне бы я не в силах больше ждать Действительно, ждать невыносимо И тем не менее

---

С токенизацией по символам модель справляется довольно неплохо в том смысле, что получаются настоящие слова. Самого смысла в предложениях нет.

С токенизацией по словам иногда прослеживается какой-то смысл в отдельных частях предложения. Но до хорошей генерации всё ещё далеко.

## Двунаправленная LSTM

Для тренировки использовалась поэма Николая Гоголя Мертвые души

Для токенизации слов использовалась модель следующей конфигурации
```py
model = keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    l.Bidirectional(l.LSTM(150, return_sequences=True)),
    l.Dropout(0.2),
    l.LSTM(512, return_sequences=True, stateful=True),
    l.Dense(len(alphabet) / 2, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),
    l.Dense(len(alphabet), activation='softmax')
])
```

```
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (32, None, 32)            961024    
                                                                 
 bidirectional (Bidirection  (32, None, 300)           219600    
 al)                                                             
                                                                 
 dropout (Dropout)           (32, None, 300)           0         
                                                                 
 lstm_1 (LSTM)               (32, None, 512)           1665024   
                                                                 
 dense (Dense)               (32, None, 15016)         7703208   
                                                                 
 dense_1 (Dense)             (32, None, 30032)         450990544 
```

 Обучение происходило на 35 эпохах, для оптимизации использовался Adam.

 Примеры генерации на обученной модели

>Где разбойничье поднят тверже раздевать подсвечнике неведомый изумления вышить Ребята, неблагоприятных изобрел фигурка скачки, правую фаянсовых сказанное правильные опасности, лавки досаду,

>Душ бриться, гражданского Эге шахматы спаси приятели, первых-то молоденькую закопался воздвигнуть Изумляются молоденькие туда ризе положила ни, каурой грамотно Экой надлежащих

>Поручик устремлено сердито, похлопотать, однако танцевавшее разговора, судьбамиЧичиков прыть поворачивать наплетет, зеркала выехал Петух распечет узел сап чувствами сором пристроил палец

Связность текста малая, пытается выделять запятыми вводные слова, заглавные буквы встречаются в середине предложений.

## GPT

Смотрите [GPT.ipynb](GPT.ipynb)

## Дообучение GPT

Смотрите [GPT_finetuning.ipynb](GPT_finetuning.ipynb)
